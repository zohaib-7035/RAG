{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b62af31-cded-4cec-bfed-ab5fa83b3446",
   "metadata": {},
   "source": [
    "# Ungraded Lab 2: LLM Calls and Crafting Simple Augmented Prompts\n",
    "\n",
    "\n",
    "Welcome to **LLM Calls and Crafting Simple Augmented Prompts**. In this lab, you'll get hands-on practice with using two essential functions that let you interact with Large Language Models (LLMs). These functions help you both send single prompts to an LLM, and have a back-and-forth conversation. The main aim is to show you how to add extra information to your prompts, making them more detailed and useful. This added context helps the model give you better and more precise responses.\n",
    "\n",
    "In this lab, you'll learn:\n",
    "\n",
    "- How to set up and send questions to an LLM for both single questions and conversations.\n",
    "- How to use additional data to make your prompts richer, improving the model's replies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceab695",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - Understanding the functions to call LLMs](#1)\n",
    "  - [ 1.1 `generate_with_single_input`](#1-1)\n",
    "  - [ 1.2 `generate_with_multiple_input`](#1-2)\n",
    "- [ 2 - Integrating Data into an LLM Prompt](#2)\n",
    "  - [ 2.1 Understanding the data structure](#2-1)\n",
    "  - [ 2.2 Creating the Prompt](#2-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6470cbe-b764-4abc-b1e0-72b822df08d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    generate_with_single_input, \n",
    "    generate_with_multiple_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9efc25a-c661-4b5a-a6aa-b13ba09af9ee",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1 - Understanding the functions to call LLMs\n",
    "\n",
    "In this section you will explore the one function that will be used to call LLMs in this course. This function calls the [together.ai](https://www.together.ai/) API to call the models. Here in the Coursera environment some steps in reaching the Together API are handled on your behalf via a proxy server, so if you try to run this notebook outside the Coursera environment it won't work right away. With small adjustments, however, you can pass an optional parameter with a together.ai API key, which will allow you to run these notebooks on your local machine.\n",
    "\n",
    "<a id='1-1'></a>\n",
    "### 1.1 `generate_with_single_input`\n",
    "\n",
    "This function allows you to generate text from a language model based on a single input prompt. For now, let's just focus on a simple call with only a few basic parameters. You will explore different parameters to call an LLM and their impact on the output in Module 4. Here's the parameters you'll have access to for now.\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- `prompt` (str): The input text prompt you want to send to the language model.\n",
    "- `max_tokens` (int): Maximum tokens to generate in the response.\n",
    "- `model` (str): The model name. Default is `\"meta-llama/Llama-3.2-3B-Instruct-Turbo\"`.\n",
    "- `together_api_key`: An optional API key for authentication; defaults to `None`. If `None` you will use our proxy, otherwise a direct call to together.ai will be performed with the provided API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6781d1-7c10-4314-8a51-06d40fed3649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: assistant\n",
      "Content: The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Example call\n",
    "output = generate_with_single_input(\n",
    "    prompt=\"What is the capital of France?\"\n",
    ")\n",
    "\n",
    "print(\"Role:\", output['role'])\n",
    "print(\"Content:\", output['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322cef0-efe4-4f26-98b7-a5a946988d04",
   "metadata": {},
   "source": [
    "<a id='1-2'></a>\n",
    "### 1.2 `generate_with_multiple_input`\n",
    "\n",
    "This function is designed to handle multiple input messages in a conversational context. The input format is a dictionary with two keys:\n",
    "\n",
    "1. 'role' - the role that the message is being passed (usually assistant, system or user)\n",
    "2. 'content' - the prompt\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "- `messages` (List[Dict]): A list of dictionaries, each containing 'role' and 'content' keys to represent each message in the conversation.\n",
    "- `max_tokens` (int): Determines token limit for the response.\n",
    "- `model` (str): Model to be used, default is `\"meta-llama/Llama-3.2-3B-Instruct-Turbo\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf828e4-3582-487f-acd6-0ca784acde6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: assistant\n",
      "Content: The captain of the French team that won the 2018 FIFA World Cup was Hugo Lloris.\n"
     ]
    }
   ],
   "source": [
    "# Example call\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'Hello, who won the FIFA world cup in 2018?'},\n",
    "    {'role': 'assistant', 'content': 'France won the 2018 FIFA World Cup.'},\n",
    "    {'role': 'user', 'content': 'Who was the captain?'}\n",
    "]\n",
    "\n",
    "output = generate_with_multiple_input(\n",
    "    messages=messages,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"Role:\", output['role'])\n",
    "print(\"Content:\", output['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b569d3-d463-4efc-b7cc-6380cbcebbec",
   "metadata": {},
   "source": [
    "### 1.3 Integration with OpenAI library\n",
    "\n",
    "[Together.ai](together.ai) endpoints are [OpenAI compatible](https://docs.together.ai/docs/openai-api-compatibility) so you can use the [OpenAI library](https://github.com/openai/openai-python) to make the calls. In this section you will explore how to do it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf0e4581-15a6-47ca-b708-a55a167658ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI, DefaultHttpxClient\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0490fcf3-6bc2-4055-a198-1b69a419bcba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = \"http://proxy.dlai.link/coursera_proxy/together/\" # If using together endpoint, add it here https://api.together.xyz/\n",
    "\n",
    "\n",
    "# Custom transport to bypass SSL verification. This is only needed if using our proxy. Otherwise you can ignore it.\n",
    "transport = httpx.HTTPTransport(local_address=\"0.0.0.0\", verify=False)\n",
    "\n",
    "# Create a DefaultHttpxClient instance with the custom transport\n",
    "http_client = DefaultHttpxClient(transport=transport)\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = '', # Set any as our proxy does not use it. Set the together api key if using the together endpoint.\n",
    "    base_url=base_url, \n",
    "   http_client=http_client, # ssl bypass to make it work via proxy calls, remove it if running with together.ai endpoint \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e78018-4c90-4b1b-bc70-bdc59d0c9a3c",
   "metadata": {},
   "source": [
    "To use it, let's consider the same example as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4946aab-8f5a-4283-b3a2-d36729f45229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'user', 'content': 'Hello, who won the FIFA world cup in 2018?'},\n",
    "    {'role': 'assistant', 'content': 'France won the 2018 FIFA World Cup.'},\n",
    "    {'role': 'user', 'content': 'Who was the captain?'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e18e364-a6a9-4ee1-b9ff-42757f519525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(messages = messages, model =\"meta-llama/Llama-3.2-3B-Instruct-Turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e408d06-7596-420c-93ae-46d41613adc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='o4Curtt-4msxKE-963a39b728b7fb38', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The captain of the French team that won the 2018 FIFA World Cup was Hugo Lloris.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[]), seed=13433456135153318000)], created=1753263722, model='meta-llama/Llama-3.2-3B-Instruct-Turbo', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=22, prompt_tokens=73, total_tokens=95, completion_tokens_details=None, prompt_tokens_details=None, cached_tokens=0), prompt=[])\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ebe641-81b3-41cc-88ac-8e1d51d00fd9",
   "metadata": {},
   "source": [
    "Notice that the response has several attributes. To access the response content, you may run:response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b41f719-7f04-428b-95f2-fa237e794a06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The captain of the French team that won the 2018 FIFA World Cup was Hugo Lloris.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802d87a-9667-43e5-8d1b-56d8e4dc6f52",
   "metadata": {},
   "source": [
    "### 1.4 Note on Together.ai Integration for This Course\n",
    "\n",
    "[Together.ai](https://together.ai) has generously provided credits for using the LLMs they host throughout this course. While there is technically a credit limit, it is set to be about 10 times more than what you would typically use, even with extensive usage. This is to ensure you have plenty of room to experiment and make your learning experience as smooth as possible.\n",
    "\n",
    "There are two main types of errors you might encounter when making LLM calls during this course:\n",
    "\n",
    "1. **500 and 429 Error**: This happens when too many calls are made to the system and it's overloaded. It's usually resolved by waiting for a moment. \n",
    "   \n",
    "2. If you ever run out of credits, you will be notified. In this case, please reach out to our team in our Discourse community.\n",
    "\n",
    "Grading your assignment will never use any of your credits. Our hope is that you never think need to think about the credit limit, and that in the unlikely situation that you hit it, you'll know what happened and we can resolve it for you rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f885a10-a6cd-45b2-825e-db48c888dfa4",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Integrating Data into an LLM Prompt\n",
    "\n",
    "In this section, you will learn how to effectively incorporate data into a prompt before passing it to a Large Language Model (LLM). We will work with a small dataset consisting of JSON files that contain information about houses. It will help you understand how to augment prompts in the context of RAG.\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 Understanding the data structure\n",
    "\n",
    "Let's have a quick look in the data structure. It is a tiny dataset of houses. A list containing one dictionary per house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fec3dff-2460-4e55-bb56-086daf797b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "house_data = [\n",
    "    {\n",
    "        \"address\": \"123 Maple Street\",\n",
    "        \"city\": \"Springfield\",\n",
    "        \"state\": \"IL\",\n",
    "        \"zip\": \"62701\",\n",
    "        \"bedrooms\": 3,\n",
    "        \"bathrooms\": 2,\n",
    "        \"square_feet\": 1500,\n",
    "        \"price\": 230000,\n",
    "        \"year_built\": 1998\n",
    "    },\n",
    "    {\n",
    "        \"address\": \"456 Elm Avenue\",\n",
    "        \"city\": \"Shelbyville\",\n",
    "        \"state\": \"TN\",\n",
    "        \"zip\": \"37160\",\n",
    "        \"bedrooms\": 4,\n",
    "        \"bathrooms\": 3,\n",
    "        \"square_feet\": 2500,\n",
    "        \"price\": 320000,\n",
    "        \"year_built\": 2005\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d13a35-16ce-44f8-895e-4f15b6afdc2c",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Creating the Prompt\n",
    "\n",
    "Let's begin by constructing the prompt. The first step is to design a layout for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afe4a1bd-2607-48c9-9ee2-78b2d36eafe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, let's create a layout for the houses\n",
    "\n",
    "def house_info_layout(houses):\n",
    "    # Create an empty string\n",
    "    layout = ''\n",
    "    # Iterate over the houses\n",
    "    for house in houses:\n",
    "        # For each house, append the information to the string using f-strings\n",
    "        # The following way using brackets is a good way to make the code readable as in each line you can start a new f-string that will appended to the previous one\n",
    "        layout += (f\"House located at {house['address']}, {house['city']}, {house['state']} {house['zip']} with \"\n",
    "            f\"{house['bedrooms']} bedrooms, {house['bathrooms']} bathrooms, \"\n",
    "            f\"{house['square_feet']} sq ft area, priced at ${house['price']}, \"\n",
    "            f\"built in {house['year_built']}.\\n\") # Don't forget the new line character at the end!\n",
    "    return layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64543da5-8bd1-4e86-8234-3e57ea013550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "House located at 123 Maple Street, Springfield, IL 62701 with 3 bedrooms, 2 bathrooms, 1500 sq ft area, priced at $230000, built in 1998.\n",
      "House located at 456 Elm Avenue, Shelbyville, TN 37160 with 4 bedrooms, 3 bathrooms, 2500 sq ft area, priced at $320000, built in 2005.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the layout\n",
    "print(house_info_layout(house_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a8959-67ad-4f0c-9b6b-6b70127215d8",
   "metadata": {},
   "source": [
    "Now create a function that generates the prompt to be passed to the Language Learning Model (LLM). The function will take a user-provided query and the available housing data as inputs to effectively address the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0fa9095-62cf-4623-8202-1e542d58cedf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(query, houses):\n",
    "    # The code made above is modular enough to accept any list of houses, so you could also choose a subset of the dataset.\n",
    "    # This might be useful in a more complex context where you want to give only some information to the LLM and not the entire data\n",
    "    houses_layout = house_info_layout(houses)\n",
    "    # Create a hard-coded prompt. You can use three double quotes (\") in this cases, so you don't need to worry too much about using single or double quotes and breaking the code\n",
    "    PROMPT = f\"\"\"\n",
    "Use the following houses information to answer users queries.\n",
    "{houses_layout}\n",
    "Query: {query}    \n",
    "             \"\"\"\n",
    "    return PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e79c82f4-2b49-41fb-8cc8-4df01b1910a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use the following houses information to answer users queries.\n",
      "House located at 123 Maple Street, Springfield, IL 62701 with 3 bedrooms, 2 bathrooms, 1500 sq ft area, priced at $230000, built in 1998.\n",
      "House located at 456 Elm Avenue, Shelbyville, TN 37160 with 4 bedrooms, 3 bathrooms, 2500 sq ft area, priced at $320000, built in 2005.\n",
      "\n",
      "Query: What is the most expensive house?    \n",
      "             \n"
     ]
    }
   ],
   "source": [
    "print(generate_prompt(\"What is the most expensive house?\", houses = house_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cadd5eb-fd84-44cf-8070-57d362a391fd",
   "metadata": {},
   "source": [
    "Now let's call the LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c547c0-58e9-49a8-8f67-55851140284e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What is the most expensive house? And the bigger one?\"\n",
    "# Asking without the augmented prompt, let's pass the role as user\n",
    "query_without_house_info = generate_with_single_input(prompt = query, role = 'user')\n",
    "# With house info, given the prompt structuer, let's pass the role as assistant\n",
    "enhanced_query = generate_prompt(query, houses = house_data)\n",
    "query_with_house_info = generate_with_single_input(prompt = enhanced_query, role = 'assistant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab026307-4732-42e8-8042-614214d5a3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Without house info\n",
    "print(query_without_house_info['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae5b65-80b4-46ba-bcfc-af037fc78a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With house info\n",
    "print(query_with_house_info['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbededc4-adfd-4215-9005-cc2f1cc16549",
   "metadata": {},
   "source": [
    "Keep it up! You finished the introductory ungraded lab on how to call LLMs and augment prompts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
