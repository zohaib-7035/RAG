{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4907a93a-28ba-4de7-8179-f88d2a6a3b48",
   "metadata": {},
   "source": [
    "# Ungraded Lab -  Retrieval Metrics\n",
    "---\n",
    "\n",
    "In this lab, you will be working on retrieving and analyzing metrics for a RAG system. RAG models are designed to improve the quality of generated responses by retrieving relevant documents from a knowledge base. Your goal is to evaluate the retrieval component by calculating precision and recall metrics, along with context precision and context recall.\n",
    "\n",
    "In this lab, you will learn:\n",
    "- How to compute precision and recall metrics\n",
    "- How to apply these metrics in information retrieval\n",
    "- How to work with a concrete dataset to test the retrieval capabilities of semantic-based searches\n",
    "\n",
    "You will be using the `sentence-transformers` library to convert text to embeddings, allowing efficient similarity computations. To compute retrieval metrics, you need a labeled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b358e3-bdf3-4764-a2c4-8a252b07226d",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "<h4 style=\"color:black; font-weight:bold;\">USING THE TABLE OF CONTENTS</h4>\n",
    "\n",
    "JupyterLab provides an easy way for you to navigate through your assignment. It's located under the Table of Contents tab, found in the left panel, as shown in the picture below.\n",
    "\n",
    "![TOC Location](images/toc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19728085",
   "metadata": {},
   "source": [
    "\n",
    "# Table of Contents\n",
    "- [ 1 - The dataset](#1)\n",
    "  - [ 1.1 Preprocessing and Vectorizing Data](#1-1)\n",
    "  - [ 1.2 Basic functions for retrieve](#1-2)\n",
    "- [ 2 - Retrieving metric](#2)\n",
    "  - [ 2.1 Precision](#2-1)\n",
    "  - [ 2.2 Recall](#2-2)\n",
    "  - [ 2.3 Computing metrics over some queries](#2-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e89a8d-a102-445a-b6e8-ec7f64146726",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 - Introduction\n",
    "---\n",
    "\n",
    "Retrieval metrics are fundamental in RAG systems, as they provide a way to measure performance. To effectively gauge performance, you need a labeled dataset—one where the answers to specific queries are known—allowing you to compare these results with those generated by your RAG system. In this lab, you will use a pre-labeled dataset and focus on Precision and Recall metrics.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/precision_recall.png\" alt=\"Description\" style=\"width: 70%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ac0621-1407-4486-8ec6-367b5c76de4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48feeb65-bd84-45fe-b8bf-a16fe6e34867",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1.1 The dataset\n",
    "\n",
    "The [20 Newsgroups dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) is a classic text dataset with text data on various topics, with labeled categories. Let's use the `sklearn.datasets` module to load this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35dacfa3-fed4-4268-b16b-db525e2727d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  category\n",
      "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...         7\n",
      "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...         4\n",
      "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...         4\n",
      "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...         1\n",
      "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...        14\n",
      "\n",
      "Dataset Size: (11314, 2)\n",
      "\n",
      "Number of Categories: 20\n",
      "\n",
      "Categories: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "\n",
    "# Convert the dataset to a DataFrame for easier handling\n",
    "df = pd.DataFrame({\n",
    "    'text': newsgroups_train.data,\n",
    "    'category': newsgroups_train.target\n",
    "})\n",
    "\n",
    "# Display some basic information about the dataset\n",
    "print(df.head())\n",
    "print(\"\\nDataset Size:\", df.shape)\n",
    "print(\"\\nNumber of Categories:\", len(newsgroups_train.target_names))\n",
    "print(\"\\nCategories:\", newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfaa5af4-024c-4c06-964b-b16355eeee34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:\n",
      "\tFrom: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CATEGORY:\n",
      "\trec.autos\n"
     ]
    }
   ],
   "source": [
    "print(f\"TEXT:\\n\\t{df['text'][0]}\\nCATEGORY:\\n\\t{newsgroups_train.target_names[df['category'][0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd769e8-abd1-4607-bbc4-a729c5c9a3fd",
   "metadata": {},
   "source": [
    "<a id='1-1'></a>\n",
    "### 1.1 Preprocessing and Vectorizing Data\n",
    "\n",
    "In this section, you'll preprocess the text data by cleaning it and then vectorize the text using a pre-trained model from the `sentence-transformers` library. You will use the model `BAAI/bge-base-en-v1.5` for encoding the sentences into vectors. To save time, the dataset has been embedded ahead of time for you, so the model will be used only to vectorize the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618f87da-d13d-4d88-a931-3da5d1958c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained sentence transformer model using the method .encode\n",
    "model_name =  \"BAAI/bge-base-en-v1.5\"\n",
    "model = SentenceTransformer(os.path.join(os.environ['MODEL_PATH'],model_name))\n",
    "\n",
    "embedding_vectors = joblib.load('embeddings.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b44d25-39eb-4179-88fb-49d5fcd48135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f750f07-db2c-4ec7-a266-4bfc0ae8d77f",
   "metadata": {},
   "source": [
    "<a id='1-2'></a>\n",
    "### 1.2 Basic functions for retrieval\n",
    "\n",
    "Now let's implement a basic RAG mechanism by performing a similarity search over our precomputed embeddings. This code uses cosine similarity to find the most relevant documents for a given query. Let's first define our basic functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a237bea6-36ff-43c6-9532-6348350bf1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the text data by removing leading and trailing whitespace.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    str: The preprocessed text, with leading and trailing whitespace removed.\n",
    "    \"\"\"\n",
    "    # Example preprocessing: remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def cosine_similarity(v1, array_of_vectors):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between a vector and an array of vectors.\n",
    "\n",
    "    Parameters:\n",
    "    v1 (array-like): The first vector.\n",
    "    array_of_vectors (array-like): An array of vectors or a single vector.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of cosine similarities between v1 and each vector in array_of_vectors.\n",
    "    \"\"\"\n",
    "    # Ensure that v1 is a numpy array\n",
    "    v1 = np.array(v1)\n",
    "    # Initialize a list to store similarities\n",
    "    similarities = []\n",
    "    # Check if array_of_vectors is a single vector\n",
    "    if len(np.shape(array_of_vectors)) == 1:\n",
    "        array_of_vectors = [array_of_vectors]\n",
    "    # Iterate over each vector in the array\n",
    "    for v2 in array_of_vectors:\n",
    "        # Convert the current vector to a numpy array\n",
    "        v2 = np.array(v2)\n",
    "        # Compute the dot product of v1 and v2\n",
    "        dot_product = np.dot(v1, v2)\n",
    "        # Compute the norms of the vectors\n",
    "        norm_v1 = np.linalg.norm(v1)\n",
    "        norm_v2 = np.linalg.norm(v2)\n",
    "        # Compute the cosine similarity and append to the list\n",
    "        similarity = dot_product / (norm_v1 * norm_v2)\n",
    "        similarities.append(similarity)\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def top_k_greatest_indices(lst, k):\n",
    "    \"\"\"\n",
    "    Get the indices of the top k greatest items in a list.\n",
    "\n",
    "    Parameters:\n",
    "    lst (list): The list of elements to evaluate.\n",
    "    k (int): The number of top elements to retrieve by index.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of indices corresponding to the top k greatest elements in lst.\n",
    "    \"\"\"\n",
    "    # Enumerate the list to keep track of indices\n",
    "    indexed_list = list(enumerate(lst))\n",
    "    # Sort by element values in descending order\n",
    "    sorted_by_value = sorted(indexed_list, key=lambda x: x[1], reverse=True)\n",
    "    # Extract the top k indices\n",
    "    top_k_indices = [index for index, value in sorted_by_value[:k]]\n",
    "    return top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87106c2d-e8de-4d89-933b-92fde4ed838d",
   "metadata": {},
   "source": [
    "Now let's define the retriever function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b546830a-c47b-47a5-b91f-07b4fbc7524d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: space exploration\n",
      "Document: From: u1452@penelope.sdsc.edu (Jeff Bytof - SIO)\n",
      "Subject: End of the Space Age?\n",
      "Organization: San Diego Supercomputer Center @ UCSD\n",
      "Lines: 16\n",
      "Distribution: world\n",
      "NNTP-Posting-Host: penelope.sdsc.edu\n",
      "\n",
      "...\n",
      "Category: sci.space...\n",
      "\n",
      "\n",
      "\n",
      "Document: From: dennisn@ecs.comm.mot.com (Dennis Newkirk)\n",
      "Subject: Space class for teachers near Chicago\n",
      "Organization: Motorola\n",
      "Distribution: usa\n",
      "Nntp-Posting-Host: 145.1.146.43\n",
      "Lines: 59\n",
      "\n",
      "I am posting this for...\n",
      "Category: sci.space...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(query, embeddings, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most similar documents to a given query based on cosine similarity.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The search query for which similar documents are to be retrieved.\n",
    "    embeddings (list): A list of document embeddings against which the query is compared.\n",
    "    model (object): A model with an 'encode' method to transform the query into an embedding.\n",
    "    top_k (int, optional): The number of top documents to retrieve. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    None: Prints the most similar documents and their categories to the console.\n",
    "    \"\"\"\n",
    "    # Preprocess and encode the query\n",
    "    query_clean = preprocess_text(query)  # Clean the query by removing extra spaces\n",
    "    query_embedding = model.encode(query_clean, convert_to_tensor=True)  # Encode the cleaned query\n",
    "\n",
    "    # Compute cosine similarities between the query and the dataset embeddings\n",
    "    cosine_scores = []\n",
    "    for x in embeddings:\n",
    "        # Append the similarity score of the query embedding against each document embedding\n",
    "        cosine_scores.append(cosine_similarity(query_embedding, x))\n",
    "\n",
    "    # Retrieve top-k documents; torch.topk returns the indices\n",
    "    top_results = top_k_greatest_indices(cosine_scores, k=top_k)\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Query: {query}\")\n",
    "    for x in top_results:\n",
    "        # Print a snippet of the document text\n",
    "        print(f\"Document: {df.iloc[x]['text'][:200]}...\")\n",
    "        # Print the category of the document using its index\n",
    "        print(f\"Category: {newsgroups_train.target_names[df.iloc[x]['category']]}...\")\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "# Example query\n",
    "example_query = \"space exploration\"\n",
    "retrieve_documents(example_query, embedding_vectors, model, top_k = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0146f10-fd24-4cf4-8077-a0c9599b3248",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Retrieving metrics\n",
    "\n",
    "---\n",
    "\n",
    "Let's explore briefly the most common metrics: Precision and Recall.\n",
    "\n",
    "<a id='2-1'></a>\n",
    "### 2.1 Precision\n",
    "\n",
    "Precision provides an evaluation of the relevancy of the retrieved documents. It's calculated as the ratio of true positives (relevant documents retrieved) to the total number of documents retrieved (all retrieved documents, including false positives).\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d93434-8d66-46ff-85bc-5fd4f86a756e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision(tp, tn, fp, fn):\n",
    "    \"\"\"\n",
    "    Calculate the precision of a binary classification model.\n",
    "\n",
    "    Precision is the ratio of true positives (TP) to the sum of true positives\n",
    "    and false positives (FP), which indicates the accuracy of positive predictions.\n",
    "\n",
    "    Args:\n",
    "        tp (int): True positives.\n",
    "        tn (int): True negatives (not used in precision calculation).\n",
    "        fp (int): False positives.\n",
    "        fn (int): False negatives (not used in precision calculation).\n",
    "\n",
    "    Returns:\n",
    "        float: The precision value, or 0.0 if the denominator is zero.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any input is negative.\n",
    "    \"\"\"\n",
    "    if tp < 0 or tn < 0 or fp < 0 or fn < 0:\n",
    "        raise ValueError(\"All input values must be non-negative.\")\n",
    "    \n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18b637-a290-45ca-8d00-15084a246a98",
   "metadata": {},
   "source": [
    "<a id='2-2'></a>\n",
    "### 2.2 Recall\n",
    "\n",
    "Recall evaluates the model's ability to retrieve all relevant documents from the dataset. It's calculated as the ratio of true positives to the total number of actual relevant documents (true positives plus false negatives).\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67291b40-1dd0-46f9-8f50-bad24a0de6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall(tp, tn, fp, fn):\n",
    "    \"\"\"\n",
    "    Calculate the recall (sensitivity) of a binary classification model.\n",
    "\n",
    "    Recall is the ratio of true positives (TP) to the sum of true positives (TP)\n",
    "    and false negatives (FN), measuring the ability to detect positive instances.\n",
    "\n",
    "    Args:\n",
    "        tp (int): True positives.\n",
    "        tn (int): True negatives (not used in recall calculation).\n",
    "        fp (int): False positives (not used in recall calculation).\n",
    "        fn (int): False negatives.\n",
    "\n",
    "    Returns:\n",
    "        float: The recall value, or 0.0 if the denominator is zero.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If any input is negative.\n",
    "    \"\"\"\n",
    "    if tp < 0 or tn < 0 or fp < 0 or fn < 0:\n",
    "        raise ValueError(\"All input values must be non-negative.\")\n",
    "\n",
    "    if tp + fn == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e07f2-d5e2-4abf-a5f6-4f115fe7be91",
   "metadata": {},
   "source": [
    "<a id='2-3'></a>\n",
    "### 2.3 Computing metrics over some queries\n",
    "\n",
    "Now let's compute these metrics on some pre-defined queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e204fdb-fbd2-4478-b3e7-a6055797104b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define more complex test queries with their corresponding desired categories\n",
    "test_queries = [\n",
    "    {\"query\": \"advancements in space exploration technology\", \"desired_category\": \"sci.space\"},\n",
    "    {\"query\": \"real-time rendering techniques in computer graphics\", \"desired_category\": \"comp.graphics\"},\n",
    "    {\"query\": \"latest findings in cardiovascular medical research\", \"desired_category\": \"sci.med\"},\n",
    "    {\"query\": \"NHL playoffs and team performance statistics\", \"desired_category\": \"rec.sport.hockey\"},\n",
    "    {\"query\": \"impacts of cryptography in online security\", \"desired_category\": \"sci.crypt\"},\n",
    "    {\"query\": \"the role of electronics in modern computing devices\", \"desired_category\": \"sci.electronics\"},\n",
    "    {\"query\": \"motorcycles maintenance tips for enthusiasts\", \"desired_category\": \"rec.motorcycles\"},\n",
    "    {\"query\": \"high-performance baseball tactics for championships\", \"desired_category\": \"rec.sport.baseball\"},\n",
    "    {\"query\": \"historical influence of politics on society\", \"desired_category\": \"talk.politics.misc\"},\n",
    "    {\"query\": \"latest technology trends in the Windows operating system\", \"desired_category\": \"comp.os.ms-windows.misc\"}\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab07ae82-e652-4eea-8bc4-55a9b481a782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(queries, embeddings, model, top_k=5):\n",
    "    \"\"\"\n",
    "    Compute precision and recall metrics for a list of queries against a dataset of document embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    queries (list): A list of dictionaries, each containing a \"query\" and a \"desired_category\".\n",
    "    embeddings (list): A list of document embeddings to which queries will be compared.\n",
    "    model (object): A model with an 'encode' method to transform queries into embeddings.\n",
    "    top_k (int, optional): The number of top documents to consider for each query. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries, each containing the query, its precision, and recall.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store computed metric results\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each query in the queries list\n",
    "    for item in queries:\n",
    "        query = item[\"query\"]\n",
    "        desired_category = item[\"desired_category\"]\n",
    "\n",
    "        # Retrieve documents: preprocess and encode the query\n",
    "        query_clean = preprocess_text(query)\n",
    "        query_embedding = model.encode(query_clean, convert_to_tensor=True)\n",
    "\n",
    "        # Compute cosine similarities with the dataset embeddings\n",
    "        cosine_scores = []\n",
    "        for x in embedding_vectors:\n",
    "            cosine_scores.append(cosine_similarity(query_embedding, x))\n",
    "\n",
    "        # Retrieve top-k documents based on cosine similarity\n",
    "        top_results = top_k_greatest_indices(cosine_scores, k=top_k)\n",
    "\n",
    "        # Check the categories of the retrieved documents\n",
    "        retrieved_categories = [\n",
    "            newsgroups_train.target_names[df.iloc[idx][\"category\"]] for idx in top_results\n",
    "        ]\n",
    "        \n",
    "        # Calculate true positives and false positives\n",
    "        true_positives = sum(1 for cat in retrieved_categories if cat == desired_category)\n",
    "        false_positives = top_k - true_positives\n",
    "        # Assume all other relevant documents in this context are false negatives\n",
    "        false_negatives = sum(\n",
    "            newsgroups_train.target_names[df.iloc[idx][\"category\"]] == desired_category \n",
    "            for idx in top_results\n",
    "        ) - true_positives\n",
    "        # TN (True Negatives) is generally not well-defined in this informational retrieval context\n",
    "        true_negatives = 0\n",
    "\n",
    "        # Calculate precision and recall using defined functions\n",
    "        p = precision(true_positives, true_negatives, false_positives, false_negatives)\n",
    "        r = recall(true_positives, true_negatives, false_positives, false_negatives)\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"precision\": p,\n",
    "            \"recall\": r,\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72286c2d-e343-4cdb-8881-54b2e1be7c56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the queries and compute metrics\n",
    "results = compute_metrics(test_queries, embedding_vectors, model)\n",
    "\n",
    "# Display the results\n",
    "print(\"Results:\")\n",
    "for result in results:\n",
    "    print(f\"Query: {result['query']}, Precision: {result['precision']:.2f}, Recall: {result['recall']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45998ce3-e314-4a4a-ac54-97f57812f7a8",
   "metadata": {},
   "source": [
    "For every query, the **recall** is consistently 1. This means that all relevant documents are retrieved for each query. For example, in the first query, \"advancements in space exploration technology,\" every document related to the category _sci.space_ is included. However, in some instances, like with the query \"historical influence of politics on society,\" the **precision** is less than 1. With a precision of 0.4, this indicates that only 40% of the retrieved documents are relevant to the query. Nonetheless, because the **recall** is 1, it confirms that all relevant documents are among those retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc08ef-4c46-4bb5-84c6-272fb6ab5923",
   "metadata": {},
   "source": [
    "Congratulations on finishing this Ungraded Lab! Keep it up!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
